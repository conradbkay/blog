[
  {
    "objectID": "posts/sorting/index.html",
    "href": "posts/sorting/index.html",
    "title": "Sorting Performance",
    "section": "",
    "text": "Results are for sorting n random floats generated using Math.random() in Node.js and random.random() in Python 3.12\nPython 3.11 introduced Powersort, an improvement on Timsort, but it isn’t that much faster for most cases.\nEach result is averaged over 10 consecutive runs with newly randomized arrays each time.\nPython was 10-20x slower than NumPy, and Node.js was 2-3x slower than Python\n\nrawData = FileAttachment(\"benchmark_data.json\").json()\n\n\nlanguages = [\"Python\", \"JavaScript\", \"NumPy\"]\n\n// Create combined dataset with language labels for sorting performance, filtering out n &lt; 256\ncombinedSortData = languages.flatMap(l =&gt; rawData[l.toLowerCase()].map(d =&gt; ({language: l, time: d.sortTime, n: d.n}))).filter(d =&gt; d.n &gt;= 256)\n\nPlot.plot({\n  title: \"Sorting Performance in Python, JavaScript, and NumPy\",\n  width: 800,\n  height: 500,\n  x: {\n    label: \"Input size (random floats)\",\n    type: \"log\",\n    base: 2,\n    domain: [256, d3.max(combinedSortData, d =&gt; d.n)]\n  },\n  y: {\n    label: \"Time (ms)\",\n    type: \"log\",\n    base: 2\n  },\n  color: {\n    legend: true,\n    domain: [\"Python\", \"JavaScript\", \"NumPy\"],\n    range: [\"#d73027\", \"#377eb8\", \"#4daf4a\"]\n  },\n  marks: [\n    // Lines for each language\n    Plot.line(combinedSortData, {\n      x: \"n\", \n      y: \"time\", \n      stroke: \"language\",\n      strokeWidth: 2\n    }),\n    \n    // Dots for each data point\n    Plot.dot(combinedSortData, {\n      x: \"n\", \n      y: \"time\", \n      fill: \"language\",\n      r: 4\n    }),\n    \n    // Tooltip\n    Plot.tip(combinedSortData, Plot.pointer({\n      x: \"n\",\n      y: \"time\",\n      title: d =&gt; `${d.language}\\n${d.n.toLocaleString()} items\\n${d.time.toFixed(4)} ms`\n    }))\n      ]\n  })\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI also tested set creating (new Set(arr) and set(arr)). This is how many times faster creating a set was compared to sorting.\nNumpy is skipped since it doesn’t really have sets. There’s set operations, and .unique but that creates a sorted array.\n\ncombinedRatioData = [\"Python\", \"JavaScript\"].flatMap(l =&gt; rawData[l.toLowerCase()].map(d =&gt; ({language: l, ratio: d.ratio, n: d.n}))).filter(d =&gt; d.n &gt;= 256)\n\nPlot.plot({\n  title: \"Set Creation vs Sorting Performance Ratio: Python vs JavaScript\",\n  width: 800,\n  height: 500,\n  x: {\n    label: \"Input size (random floats)\",\n    type: \"log\",\n    base: 2,\n    domain: [256, d3.max(combinedRatioData, d =&gt; d.n)]\n  },\n  y: {\n    label: \"Ratio (sort time / set time )\",\n    type: \"linear\",\n  },\n  color: {\n    legend: true,\n    domain: [\"Python\", \"JavaScript\"],\n    range: [\"#d73027\", \"#377eb8\"]\n  },\n  marks: [\n    // Horizontal line at y=1 for reference\n    Plot.ruleY([1], {stroke: \"#999\", strokeDasharray: \"3,3\"}),\n    \n    // Lines for each language\n    Plot.line(combinedRatioData, {\n      x: \"n\", \n      y: \"ratio\", \n      stroke: \"language\",\n      strokeWidth: 2\n    }),\n    \n    // Dots for each data point\n    Plot.dot(combinedRatioData, {\n      x: \"n\", \n      y: \"ratio\", \n      fill: \"language\",\n      r: 4\n    }),\n    \n    // Tooltip\n    Plot.tip(combinedRatioData, Plot.pointer({\n      x: \"n\",\n      y: \"ratio\",\n      title: d =&gt; `${d.language}\\n${d.n.toLocaleString()} items\\nRatio: ${d.ratio.toFixed(3)}`\n    }))\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe sorting rabbit hole goes very deep, as does performance and benchmarking, but this is enough that I typically won’t worry about having to sort arrays."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Conrad Kay",
    "section": "",
    "text": "ML\n\n\nfeatured\n\n\n\n\n\n\nApr 2, 2025\n\n\n13 min\n\n\n2,422 words\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#featured",
    "href": "index.html#featured",
    "title": "Conrad Kay",
    "section": "",
    "text": "ML\n\n\nfeatured\n\n\n\n\n\n\nApr 2, 2025\n\n\n13 min\n\n\n2,422 words\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#notes-tidbits",
    "href": "index.html#notes-tidbits",
    "title": "Conrad Kay",
    "section": "Notes & Tidbits",
    "text": "Notes & Tidbits\n\n\n\n\n\n\n\nSorting Performance\n\n\n4 min\n\n\n\nMay 16, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Web developer, among other things"
  },
  {
    "objectID": "posts/aicg/index.html",
    "href": "posts/aicg/index.html",
    "title": "Deep Comic Book Grading",
    "section": "",
    "text": "Update: You can benchmark yourself against the model’s exact predictions here: https://ai.conradkay.com/grade\nAttempting this project as a web developer without a math or data science background was a long shot, but I ended up getting better results than I thought was possible."
  },
  {
    "objectID": "posts/aicg/index.html#gathering-data",
    "href": "posts/aicg/index.html#gathering-data",
    "title": "Deep Comic Book Grading",
    "section": "Gathering Data",
    "text": "Gathering Data\nGetting (good) data is the hardest part for most real-world ML projects. But there’s plenty of excellent datasets of all sizes available for free, and the process of obtaining data depends so much on the task that I get too in the weeds here. Essentially it was a lot of scraping sales listings.\nFor just CGC graded comics, I got ~2.1m pairs of images, or ~7TB of .jpg files which is rather annoying.\neBay listings have 0-12 images, each at most 1600x1600. Anyone can post on eBay so the images are very diverse and unstructured. I’m scraping the data for now, but to be usable for training they’d need additional filtering and processing."
  },
  {
    "objectID": "posts/aicg/index.html#exploratory-data-analysis-eda",
    "href": "posts/aicg/index.html#exploratory-data-analysis-eda",
    "title": "Deep Comic Book Grading",
    "section": "Exploratory Data Analysis (EDA)",
    "text": "Exploratory Data Analysis (EDA)\nSome things to note about grading:\n\nThe front and back cover are equally important, but it’s rare for the interior pages to impact the grade\nManufacturing defects are mostly ignored except for grades above 9.8, which are extremely rare\nCheaper comics aren’t worth grading unless a high grade is expected (often 9.8 or bust)\nOlder comics are much more scarce, but therefore expensive and more likely to be graded\n\nI made a bunch of visualizations and reports to understand the data better. This heatmap is probably the most succinct"
  },
  {
    "objectID": "posts/aicg/index.html#designing-a-dataset",
    "href": "posts/aicg/index.html#designing-a-dataset",
    "title": "Deep Comic Book Grading",
    "section": "Designing a Dataset",
    "text": "Designing a Dataset\nThere’s a few unique and interesting constraints here:\n\n\n\n\nA scoring based approach seemed natural\nEach year grouping is treated as a separate dataset, and an initial pass just determines the size of each.\nI made each year (range) a heap,\nThis maximizes image diversity, smooths the total distribution of grades, and limits the effectiveness of grading based on year or the specific comic\nThe validation set is created first on a random 10% of the data, so it can’t steal all the highest score examples. A better way than random splitting would be to make every data source (what site/seller) exclusive to either the training or validation set. That way it’s clear if the model doesn’t generalize to things like different camera usage, backgrounds, or scanner settings.\nI didn’t bother creating a test set since new data comes in fast enough I could just create one on the fly."
  },
  {
    "objectID": "posts/aicg/index.html#takeaways",
    "href": "posts/aicg/index.html#takeaways",
    "title": "Deep Comic Book Grading",
    "section": "Takeaways",
    "text": "Takeaways\nFor everything I eventually figured out there was much trial-and-error, looking at data, analyzing failures, and getting stuck or confused.\nThe potentially huge advantage you can have over someone much more experienced in ML is specific knowledge about some domain, or unique access to data. There’s probably not something that immediately comes to mind but maybe you know someone, or having the proverbial “hammer” will make some “nail” stick out in the future."
  },
  {
    "objectID": "posts/aicg/index.html#resources-i-found-useful",
    "href": "posts/aicg/index.html#resources-i-found-useful",
    "title": "Deep Comic Book Grading",
    "section": "Resources I found useful",
    "text": "Resources I found useful\nI made this blog with Quarto, which was easy to set up but seems very capable\nPython for Data Analysis by Wes McKinney (the creator of pandas)\n3blue1brown Deep Learning\nhttps://www.youtube.com/@AndrejKarpathy\nfast.ai Practical Deep Learning for Coders Course"
  },
  {
    "objectID": "posts/aicg/index.html#technical-details",
    "href": "posts/aicg/index.html#technical-details",
    "title": "Deep Comic Book Grading",
    "section": "Technical Details",
    "text": "Technical Details\nProbably skip this part if you don’t have PyTorch or ML experience\nI found the FastAI defaults (which haven’t changed much in 7 years) really difficult to beat. There’s quite a bit of randomness in training, so even if I could get a slight improvement it’s hard to say whether it was just luck."
  }
]