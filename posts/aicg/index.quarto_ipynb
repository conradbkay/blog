{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eadde49d",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Deep Comic Book Grading\"\n",
    "author: \"Conrad Kay\"\n",
    "date: \"2025-04-02\"\n",
    "last-modified: \"2025-06-21\"\n",
    "categories: [ML, featured]\n",
    "image: \"assets/cam-thumbnail.jpg\"\n",
    "format: html\n",
    "echo: false\n",
    "---\n",
    "\n",
    "**Update**: You can benchmark yourself against the model's exact predictions here: <https://ai.conradkay.com/grade>\n",
    "\n",
    "Attempting this project as a web developer without a math or data science background was a long shot, and I went into it with only the expectation that I'd learn a lot from my failures. I soon got better results than I thought was possible and started iterating on the initial concept.\n",
    "\n",
    "## Background\n",
    "\n",
    "This a comic graded and encapsulated by CGC, which was the first comic book grading service and remains the most popular. The only jargon I'll use is that these are \"slabbed\" comics, as opposed to a regular \"raw\" comic.\n",
    "\n",
    "::: {layout-ncol=3}\n",
    "![](assets/power2-f.jpg){height=\"25%\"}\n",
    "\n",
    "![](assets/power2-b.jpg){height=\"25%\"}\n",
    ":::\n",
    "\n",
    "There are [some](https://agscard.com/) [companies](https://taggrading.com/) which use AI to grade trading cards, but nothing has been used for comic books except asking chatbots. \n",
    "\n",
    "After trying a few popular models I began to suspect they were basically spitting out the \"average\" grade based on the comic's age. To test that I gave them some high-grade old comics, and a couple low-grade modern comics. Prompt: \"As a professional grader, use the provided scans of the front and back cover to assign a grade from 0.5 to 10.\" Prompting to describe any the defects *before* assigning a grade didn't seem to help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34952a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        </script>\n",
       "        <script type=\"module\">import \"https://cdn.plot.ly/plotly-3.0.1.min\"</script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
       "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-3.0.1.min.js\" integrity=\"sha256-oy6Be7Eh6eiQFs5M7oXuPxxm9qbJXEtTpfSI93dW16Q=\" crossorigin=\"anonymous\"></script>                <div id=\"8068fbb8-6275-41d8-a212-21c38172b2ec\" class=\"plotly-graph-div\" style=\"height:300px; width:640px;\"></div>            <script type=\"text/javascript\">                window.PLOTLYENV=window.PLOTLYENV || {};                                if (document.getElementById(\"8068fbb8-6275-41d8-a212-21c38172b2ec\")) {                    Plotly.newPlot(                        \"8068fbb8-6275-41d8-a212-21c38172b2ec\",                        [{\"customdata\":[[\"Claude 3.7 Sonnet\",1965,\"6.0\",6.0,3.4000000000000004],[\"Claude 3.7 Sonnet\",1963,\"6.0\",6.0,3.4000000000000004],[\"ChatGPT (o4-mini)\",1965,\"7.5\",7.5,1.9000000000000004],[\"ChatGPT (o4-mini)\",1963,\"5.5-6.0\",5.75,3.6500000000000004],[\"Gemini 2.5 Pro\",1944,\"5.0\",5.0,4.800000000000001],[\"Gemini 2.5 Pro\",1959,\"4.5-5.5\",5.0,4.4],[\"Gemini 2.5 Pro\",2006,\"9.6\",9.6,-3.5999999999999996],[\"Gemini 2.5 Pro\",1988,\"7.0-7.5\",7.25,-3.25]],\"hovertemplate\":\"\\u003cb\\u003eModel: %{customdata[0]}\\u003c\\u002fb\\u003e\\u003cbr\\u003eComic Year: %{customdata[1]}\\u003cbr\\u003e---\\u003cbr\\u003e\\u003cb\\u003eActual Grade: %{y:.1f}\\u003c\\u002fb\\u003e\\u003cbr\\u003e\",\"marker\":{\"color\":[\"rgb(23, 190, 207)\",\"rgb(23, 190, 207)\",\"rgb(31, 119, 180)\",\"rgb(31, 119, 180)\",\"rgb(255, 127, 14)\",\"rgb(255, 127, 14)\",\"rgb(255, 127, 14)\",\"rgb(255, 127, 14)\"],\"line\":{\"color\":\"DarkSlateGrey\",\"width\":1.5},\"size\":10,\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"Actual Grade\",\"x\":{\"dtype\":\"i1\",\"bdata\":\"AAECAwQFBgc=\"},\"y\":{\"dtype\":\"f8\",\"bdata\":\"zczMzMzMIkDNzMzMzMwiQM3MzMzMzCJAzczMzMzMIkCamZmZmZkjQM3MzMzMzCJAAAAAAAAAGEAAAAAAAAAQQA==\"},\"type\":\"scatter\"},{\"customdata\":[[\"Claude 3.7 Sonnet\",1965,9.4,\"6.0\",3.4000000000000004],[\"Claude 3.7 Sonnet\",1963,9.4,\"6.0\",3.4000000000000004],[\"ChatGPT (o4-mini)\",1965,9.4,\"7.5\",1.9000000000000004],[\"ChatGPT (o4-mini)\",1963,9.4,\"5.5-6.0\",3.6500000000000004],[\"Gemini 2.5 Pro\",1944,9.8,\"5.0\",4.800000000000001],[\"Gemini 2.5 Pro\",1959,9.4,\"4.5-5.5\",4.4],[\"Gemini 2.5 Pro\",2006,6.0,\"9.6\",-3.5999999999999996],[\"Gemini 2.5 Pro\",1988,4.0,\"7.0-7.5\",-3.25]],\"hovertemplate\":\"%{customdata[3]}\\u003cbr\\u003e\",\"marker\":{\"color\":[\"rgb(23, 190, 207)\",\"rgb(23, 190, 207)\",\"rgb(31, 119, 180)\",\"rgb(31, 119, 180)\",\"rgb(255, 127, 14)\",\"rgb(255, 127, 14)\",\"rgb(255, 127, 14)\",\"rgb(255, 127, 14)\"],\"line\":{\"color\":\"DarkSlateGrey\",\"width\":1.5},\"size\":10,\"symbol\":\"x\"},\"mode\":\"markers\",\"name\":\"Guessed Grade\",\"x\":{\"dtype\":\"i1\",\"bdata\":\"AAECAwQFBgc=\"},\"y\":{\"dtype\":\"f8\",\"bdata\":\"AAAAAAAAGEAAAAAAAAAYQAAAAAAAAB5AAAAAAAAAF0AAAAAAAAAUQAAAAAAAABRAMzMzMzMzI0AAAAAAAAAdQA==\"},\"type\":\"scatter\"},{\"legendgroup\":\"models\",\"marker\":{\"color\":\"rgb(23, 190, 207)\",\"size\":10,\"symbol\":\"square\"},\"mode\":\"markers\",\"name\":\"Claude 3.7 Sonnet\",\"showlegend\":true,\"x\":[null],\"y\":[null],\"type\":\"scatter\"},{\"legendgroup\":\"models\",\"marker\":{\"color\":\"rgb(31, 119, 180)\",\"size\":10,\"symbol\":\"square\"},\"mode\":\"markers\",\"name\":\"ChatGPT (o4-mini)\",\"showlegend\":true,\"x\":[null],\"y\":[null],\"type\":\"scatter\"},{\"legendgroup\":\"models\",\"marker\":{\"color\":\"rgb(255, 127, 14)\",\"size\":10,\"symbol\":\"square\"},\"mode\":\"markers\",\"name\":\"Gemini 2.5 Pro\",\"showlegend\":true,\"x\":[null],\"y\":[null],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scattermap\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermap\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"margin\":{\"b\":0,\"l\":0,\"r\":0,\"t\":30},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"shapes\":[{\"line\":{\"color\":\"rgb(23, 190, 207)\",\"width\":2},\"opacity\":0.7,\"type\":\"line\",\"x0\":0,\"x1\":0,\"y0\":9.4,\"y1\":6.0},{\"line\":{\"color\":\"rgb(23, 190, 207)\",\"width\":2},\"opacity\":0.7,\"type\":\"line\",\"x0\":1,\"x1\":1,\"y0\":9.4,\"y1\":6.0},{\"line\":{\"color\":\"rgb(31, 119, 180)\",\"width\":2},\"opacity\":0.7,\"type\":\"line\",\"x0\":2,\"x1\":2,\"y0\":9.4,\"y1\":7.5},{\"line\":{\"color\":\"rgb(31, 119, 180)\",\"width\":2},\"opacity\":0.7,\"type\":\"line\",\"x0\":3,\"x1\":3,\"y0\":9.4,\"y1\":5.75},{\"line\":{\"color\":\"rgb(255, 127, 14)\",\"width\":2},\"opacity\":0.7,\"type\":\"line\",\"x0\":4,\"x1\":4,\"y0\":9.8,\"y1\":5.0},{\"line\":{\"color\":\"rgb(255, 127, 14)\",\"width\":2},\"opacity\":0.7,\"type\":\"line\",\"x0\":5,\"x1\":5,\"y0\":9.4,\"y1\":5.0},{\"line\":{\"color\":\"rgb(255, 127, 14)\",\"width\":2},\"opacity\":0.7,\"type\":\"line\",\"x0\":6,\"x1\":6,\"y0\":6.0,\"y1\":9.6},{\"line\":{\"color\":\"rgb(255, 127, 14)\",\"width\":2},\"opacity\":0.7,\"type\":\"line\",\"x0\":7,\"x1\":7,\"y0\":4.0,\"y1\":7.25}],\"legend\":{\"title\":{\"text\":\"Legend\"}},\"xaxis\":{\"title\":{\"text\":\"Sample Year\"},\"tickmode\":\"array\",\"tickvals\":{\"dtype\":\"i1\",\"bdata\":\"AAECAwQFBgc=\"},\"ticktext\":[\"1965\",\"1963\",\"1965\",\"1963\",\"1944\",\"1959\",\"2006\",\"1988\"],\"tickangle\":0},\"yaxis\":{\"title\":{\"text\":\"Grade\"},\"range\":[3.5,10],\"dtick\":1.0},\"height\":300,\"width\":640,\"hovermode\":\"x unified\"},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('8068fbb8-6275-41d8-a212-21c38172b2ec');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "exec(open('llm-bench.py').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27550f8",
   "metadata": {},
   "source": [
    "<br />\n",
    "They barely did better than random guessing, and the explanations were very well-written but ultimately BS. For many tasks they have remarkable vision capabilities (see <https://www.astralcodexten.com/p/testing-ais-geoguessr-genius>), just this one doesn't gel with their training.\n",
    "\n",
    "## Gathering Data\n",
    "\n",
    "Getting high-quality data is the hardest part of many real-world ML projects. But there's plenty of excellent datasets of all sizes available for free, and the process of obtaining data depends so much on the task that I won't get too in the weeds here. Essentially it was a lot of scraping sales listings.\n",
    "\n",
    "For slabbed comics, I got ~2.5m pairs of images, or ~8TB of .jpg files which is rather annoying to deal with.\n",
    "\n",
    "<!--\n",
    "Some things to note about grading:\n",
    "\n",
    "- The front and back cover are equally important, but it's rare for the interior pages to impact the grade\n",
    "- Manufacturing defects are mostly ignored except for grades above 9.8, which are extremely rare\n",
    "- Cheaper comics aren't worth grading unless a high grade is expected (often 9.8 or bust)\n",
    "- Older comics are much more scarce, but therefore expensive and more likely to be graded\n",
    "-->\n",
    "\n",
    "## Designing a Dataset\n",
    "\n",
    "\n",
    "\n",
    "I made a ton of visualizations and reports to understand the data better. This heatmap of slabbed comics is probably the most succinct:\n",
    "\n",
    "![](assets/year-grade.jpg)\n",
    "\n",
    "There's a few interesting constraints here:\n",
    "- \n",
    "\n",
    "<!--\n",
    "At first I just limited the number of comics in each grade, then I did year+grade buckets. But it still wasn't diverse enough so I made it \n",
    "-->\n",
    "A scoring based approach seemed natural\n",
    "\n",
    "Each year grouping is treated as a separate dataset, and an initial pass just determines the size of each.\n",
    "\n",
    "I made each year (range) a heap, \n",
    "\n",
    "This maximizes image diversity, smooths the total distribution of grades, and limits the effectiveness of grading based on year or the specific comic\n",
    " \n",
    "![](assets/year-grade-ds.jpg)\n",
    "\n",
    "The validation set is created first from a random 10% of the data, so it can't steal all the highest score examples. A better way than random splitting would be to make every data source (what site/seller) exclusive to either the training or validation set. That way it's clear if the model doesn't generalize to factors like different camera usage, backgrounds, or scanner settings.\n",
    "\n",
    "## Image Pre-processing\n",
    "\n",
    "224x224 is the most common resolution for computer vision models. But most of the combined front and back cover scans are around 4000x3000, 239 times as many pixels. The price increase will be more than linear, especially to keep similar training speed.\n",
    "\n",
    "I don't have VC money to burn, so if I don't want to be financially ruined the name of the game is to maximize information and minimize image size.\n",
    "\n",
    "### Cropping\n",
    "\n",
    "If we give a model full images of CGC comics, It can obviously cheat by looking at the big grade number, so it's necessary to remove the label, and ideally crop as close to the actual comic to remove meaningless pixels. \n",
    "\n",
    "As it turns out, I underestimated the difficulty of finding a rectangle within another rectangle.\n",
    "\n",
    "- v1: Just removing a fixed percent from each side works well enough as a baseline, as long as images are already cropped to the slab itself \n",
    "- v1.5: Non-ML methods using [OpenCV](https://opencv.org/) often failed because of several boundaries close together, or comics the same color as the background\n",
    "- v2: A [DETR](https://huggingface.co/docs/transformers/en/model_doc/detr) object detection model. Requires manually labeling some images with bounding boxes. \n",
    "- v3: Training a U-Net model on top of a segmentation backbone to produce one heatmap for each corner and convert them into points. By assuming the comic is a rectangle perspective warping is applied\n",
    "\n",
    "The U-Net model's code was fairly complicated, but it outputs points on average 0.15% away from what I selected, or roughly a pixel off in it's input images. It's probably more consistent than I am, very impressive since it only uses a `tiny` SAM 2.1 backbone and images smaller than it was originally trained at.\n",
    "\n",
    "This is the aspect ratios for the resulting crops:\n",
    "![](assets/aspect.jpg)\n",
    "\n",
    "Out of the popular annotation tools, I found [label-studio](https://github.com/HumanSignal/label-studio) to be the easiest, but it still frustrated me quite a bit. I ended up building my own web app to do the point annotation, which took maybe 2 hours in total since LLMs are good at the bulk of trivial projects like that. \n",
    "\n",
    "![](assets/label.jpg)\n",
    "\n",
    "It has all the features I want, and only the features I want. The biggest advantage is that I have full control over the input/output. I can write any query for what to label, and the output is written directly to the database in a usable format.\n",
    "\n",
    "### Mangling\n",
    "\n",
    "Most of the grade impact is in the pixels at the edges of each cover, so the simplest approach is to only use the edges. Unfortunately there's no easy way to remove the center of an image like there is by cropping to remove sides.\n",
    "\n",
    "My approach was to divide each cover into 4 sides. A fixed amount `x` controls how far \n",
    "\n",
    "Doing this for both covers, there's 2 sets of 4 images. Images are oriented to be vertical, scaled to the same height, and mirrored so that the edges are on the left.\n",
    "\n",
    "::: {.columns style=\"align-items: stretch;\"}\n",
    "\n",
    "::: {.column width=\"34%\"}\n",
    "![](assets/flash-uncrop.jpg){width=\"100%\"}\n",
    "<!--\n",
    "![](assets/flash-uncrop-2.jpg){width=\"100%\"}\n",
    "-->\n",
    ":::\n",
    "\n",
    "::: {.column width=\"39%\"}\n",
    "![](assets/flash-crop.jpg){width=\"100%\"}\n",
    "<!--\n",
    "![](assets/flash-crop-2.jpg){width=\"100%\"}\n",
    "-->\n",
    ":::\n",
    "\n",
    "::: {.column width=\"27%\"}\n",
    "![](assets/front-cols.jpg){width=\"100%\"}\n",
    ":::\n",
    "\n",
    ":::\n",
    "\n",
    "Later I experimented with adding the centers, but proportionally smaller. Metrics didn't improve as much as I hoped, so I'll have to investigate whether the different scale within an input is hard for it to learn, the center matters less than I thought, or the defects there need higher resolution to detect.\n",
    "\n",
    "If instead the left and right side include the corners, using `x=(2w-h)/4` means the top/bottom are exactly half as large as the left/right. \n",
    "\n",
    "We can either produce 6 columns per cover by splitting the left and right in half, or 3 columns and 2 rows by stacking the top and bottom vertically.\n",
    "\n",
    "![](assets/front-cols-v2.jpg)\n",
    "\n",
    "Only the center is resized, so it's possible to reconstruct the rest of the image perfectly, then fit the center into the missing space.\n",
    "\n",
    "## Training\n",
    "\n",
    "All of the models I used had already been pre-trained on millions of images. Even though most of what they learned won't be useful, anything is better than starting from scratch.\n",
    "\n",
    "[timm](https://github.com/huggingface/pytorch-image-models) is a fantastic library with implementations and weights for many different models.\n",
    "\n",
    "The DINO training method (<https://arxiv.org/abs/2104.14294>) is really interesting, and did better than all of the 20 or so other architectures I tried. dinov2-base (87m parameters) got 0.717 MAE keeping the model frozen and just training the final classifiers, a shockingly good result considering how different these images are from what it trained on.\n",
    "\n",
    "<!--\n",
    "The approach of not using any label is quite fascinating. As a crude analogy, compare it to giving a bunch of paintings to someone who doesn't know much about art, and having them sort it into groups/piles \n",
    "\n",
    "If you gave several versions of painting, with random flipping/mirroring, cropping, hue changes, etc. they'll probably do worse, but are forced to learn more robust ways to categorize paintings. Maybe you merge two paintings somehow, and expect them to place it somewhere in-between the two groups\n",
    "\n",
    "If instead of one person, it was closer to a game of telephone where nobody spoke a common language, they'd probably learn to just focus on the important parts\n",
    "-->\n",
    "\n",
    "## Benchmarking\n",
    "\n",
    "The simplest metric I'm using is mean absolute error (MAE), basically how far off a guess is on average on the 0.5-10 scale.\n",
    "\n",
    "To measure human performance, I used posts from <https://boards.cgccomics.com/forum/42-hey-buddy-can-you-spare-a-grade/> where they say the official grade once it comes back from CGC. The images tend to be very high quality with a lot of different angles. While there might be a bias towards people posting harder to grade comics, the validation set is very biased towards difficult/outlier examples. Across 34 posts there were 151 predictions, with an MAE of 0.80. Using the average guess for each post results in an 0.74 MAE.\n",
    "\n",
    "On the validation set, I graded 50 examples myself using full resolution scans, and got an MAE of 0.55. Good for a human but even dinov2_small with it's 22 million parameters beat me.\n",
    "\n",
    "It didn't feel like there was any clear weakness with the model. Analyzing the largest differences in the model's prediction and the official grade, I found myself usually siding with the model, meaning there were just defects not visible from the images. And funny enough a lot of them were due to egregious mistakes from the grading company, not the model. \n",
    "\n",
    "![]()\n",
    "\n",
    "The most obvious advantage of AI over humans is consistency. The model will always give the same grade if the same input images are used,\n",
    "\n",
    "## Grad-CAM\n",
    "\n",
    "![The front and back cover are almost identical for this comic, so it looks a bit like certain columns were duplicated](assets/cam.jpg)\n",
    "\n",
    "Generating these visualizations is fairly simple, in both code and computation:\n",
    "\n",
    "- We force PyTorch to do some calculus measuring much each region impacts the probability it gives for a specific output\n",
    "- That gets normalized and reshaped into a grayscale image matching our input image's dimensions\n",
    "- OpenCV makes it easy to use a colormap (Turbo in this case) so that instead of displaying from black to white, it shows blue to yellow to red\n",
    "\n",
    "These give a way to check the model's work. If it misses anything that seems significant, the grade is probably lower than it predicts. The model paying attention to things which aren't defects is more nuanced. For flawless comics it might focus the most on sharp corners, but it often focused somewhat on barcodes or logos.\n",
    "\n",
    "## Multi-task Learning\n",
    "\n",
    "Right now the model doesn't use any language data, but it would be nice if it could describe the defects like multimodal models do. Luckily there's an ID on each comic which can be used to visit a web page containing information about the comic and grade, and has \"grading notes\" a bit less than half the time.\n",
    "\n",
    "Usually there's 1-4 defects listed, which follow a rough format:\n",
    "\n",
    "- light spine stress lines to cover\n",
    "- multiple moderate crease back cover\n",
    "- spine stress lines breaks color\n",
    "\n",
    "In my sample there's ~20k unique descriptions, with 80% of them occuring less than 5 times. \n",
    "\n",
    "For now I'm ignoring positional information (like \"top right of back cover\") since that would increase the size by >50x, and Grad-CAM can supplement that information in the results.\n",
    "\n",
    "I extracted 31 unique defects, and converted words measuring the impact (severity, size, frequency) to a number (0-1). I did basically the same thing with the restoration info.\n",
    "\n",
    "Page color and year were more straightforward since there's a single target value.\n",
    "\n",
    "I wouldn't say it's intuitive to add this these the existing model which focuses on grading. With most physical systems (a car for example), they're either specialized and good at just a few things, or general and decent at many things. Deep learning models are closer to monsters than machines, consuming as much data as possible, and more outputs means more data. <!-- lottery ticket hypothesis, dropout, etc -->\n",
    "\n",
    "It makes more sense if instead of thinking about the grader as one model, . There's the backbone/encoder \n",
    "\n",
    "<!--\n",
    "I'm pretty sure it's cheating on restoration based on the label being purple, . Must be that somehow light is diffracting, or the camera is adjusting for the different color. Scary smart, the vibe training these things is somewhere between being in Jurassic Park and talking to Hannibal Lecter.\n",
    "-->\n",
    "\n",
    "<!--\n",
    "It would be interesting to train a model to grade just from the grader's notes. It wouldn't be that useful, but it would help to understand them more and probably lead to designing better targets for the model.\n",
    "-->\n",
    "\n",
    "## v2.0 Ideas\n",
    "\n",
    "### Contrastive Learning\n",
    "\n",
    "So far the model to being asked to somehow figure out,\n",
    "\n",
    "With the current method of dataset creation, only 2% of the training set wouldn't have a pair, or 4% for triplets\n",
    "\n",
    "![](assets/canon-count.jpg)\n",
    "\n",
    "<!-- Distillation sounds like it should be complex, but it's basically just training a model on the output probabilities of a better model. This should be quite useful for the language data because it means no defects are skipped like they might be in the actual dataset. For the grade this should help too, since there's more information in the probability distribution than just a single number-->\n",
    "\n",
    "### More Data\n",
    "\n",
    "So far the model has only been trained on slabs, which limits the available data somewhat. Another issue is that the input images are different, since the slabs have an outer plastic shell and an inner bag between the comic and camera. Glare/reflection is the biggest, but the sources I used are good at minimizing this or have expensive scanners. Scratches and scuffs\n",
    "\n",
    "By comparing the model's output on images of the same comic before and after it was graded by CGC, we can convert between slabbed and raw grades. The adjustment should be different based on the grade, since lower grade comics will have more obvious defects which don't get obscured as much.\n",
    "\n",
    "The problem with using is that every seller grades differently.\n",
    "\n",
    "Imagine we take several different graders. We can get a bias score, and a consistency score. We can arbitrarily set a consistency of 1 as equaling the model by measuring the standard deviation. The model is slightly biased towards grading too high since sometimes it doesn't catch a defect, usually since it's impossible based on the input.\n",
    "\n",
    "![](assets/bias.jpg)\n",
    "\n",
    "Basically we can create a model to convert one of their grades into the probabilities our model would output on average. So if they gave a comic an 8.5, we might output 40% 8.5, 30% 9.0, 15% 8.0, 0.4% 9.6, etc. \n",
    "\n",
    "Essentially, that converts their grading format-- which is usually the same 0.5-10, but often less precise or biased in a specific direction--into CGC's format. It's fairly unlikely. The only thing to really worry about is more specific biases. Training on worse/less applicable data first is a common theme in ML, and should greatly mitigate this. \n",
    "<!--\n",
    "### eBay\n",
    "\n",
    "eBay listings have 0-12 images, each at most 1600x1600. Anyone can post on eBay so the images are very diverse and unstructured. To be usable for training they'd need additional filtering and processing.\n",
    "-->\n",
    "\n",
    "### Confidence\n",
    "\n",
    "All of the model's final outputs are in the form of probabilities, but they seem to always have the same distribution. \n",
    "\n",
    "## Takeaways \n",
    "\n",
    "For everything I eventually figured out there was much trial-and-error, looking at data, analyzing failures, and getting stuck or confused.\n",
    "\n",
    "The potentially huge advantage you can have over someone much more experienced in ML is specific knowledge about some domain, or unique access to data. There's probably not something that immediately comes to mind but maybe you know someone, or having the proverbial \"hammer\" will make some \"nail\" stick out in the future.\n",
    "\n",
    "### Useful Resources\n",
    "\n",
    "I made this blog with [Quarto](https://quarto.org), which was easy to set up but seems very capable\n",
    "\n",
    "[Python for Data Analysis by Wes McKinney (the creator of pandas)](https://github.com/wesm/pydata-book)\n",
    "\n",
    "[3blue1brown Deep Learning](https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)\n",
    "\n",
    "<https://www.youtube.com/@AndrejKarpathy>\n",
    "\n",
    "[fast.ai Practical Deep Learning for Coders Course](https://www.youtube.com/watch?v=8SF_h3xF3cE&list=PLfYUBJiXbdtSvpQjSnJJ_PmDQB_VyT5iU)\n",
    "\n",
    "## Technical Details\n",
    "\n",
    "Probably skip this part if you don't have PyTorch or ML experience\n",
    "\n",
    "I found the FastAI defaults (which haven't changed much in 7 years) very difficult to beat. There's quite a bit of randomness in training, so even if I could get a slight improvement it's hard to say whether it was just luck.\n",
    "\n",
    "\n",
    "### Entire Process\n",
    "\n",
    "<!--\n",
    "    %% Data Acquisition\n",
    "    EBAY(Comics from eBay API)\n",
    "    EBAY -!!-> QL(\"Manual Image Type & Quality Annotation\")\n",
    "    QL -!!-> TRQL(Train Predictive Model and Run)\n",
    "    TRQL -!!-> DB\n",
    "-->\n",
    "```{mermaid}\n",
    "flowchart TD\n",
    "    SCR[Scrape Marketplaces]\n",
    "    SCR --> DB[Database]\n",
    "\n",
    "    DB --> DL[Download Images]\n",
    "    \n",
    "    DL --> ANCR[Manually Crop]\n",
    "    ANCR --> TRCR[Train DETR Crop Model]\n",
    "    TRCR --> TSFM\n",
    "\n",
    "    DL --> OCR(\"Certificate # from OCR Model\")\n",
    "    OCR --> SCR2(\"Scrape Grader's Registry (Restoration, Defects)\")\n",
    "    SCR2 --> DB\n",
    "\n",
    "\n",
    "    DB --> SORT[\"Canonization, Filtering, and Scoring\"]\n",
    "    %% Splitting\n",
    "    SORT --> DS[\"Create Validation Set (10k) then Training Sets (10k-1m)\"]\n",
    "    DS --> EDA[\"EDA (Exploratory Data Analysis)\"]\n",
    "\n",
    "    DS --> TSFM[Generate Transformed Images at Multiple Resolutions]\n",
    "    TSFM --> TR[Train Model]\n",
    "    TR --> VIS[\n",
    "      Biggest Losses\n",
    "      Bias / Skew\n",
    "      Grad-CAM Visualizations\n",
    "    ]\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3",
   "path": "/home/conrad/Projects/blog/.venv/share/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
