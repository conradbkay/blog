---
title: "Can a computer grade comic books?"
author: "Conrad Kay"
date: "2025-04-02"
categories: [ML]
image: "cam-thumbnail.png"
---

Starting this project I knew next to nothing about training a machine learning model. I'm a web developer, not particularly knowledgable in math or data science. As such, I'll try and make everything understandable with just general programming knowledge.

# Background

This a comic encapsulated and graded by CGC, which was the first comic grading service and remains the most popular

::: {layout-ncol=2}

![](power2-f.jpg)

![](power2-b.jpg)
:::

There are some companies (like AGS) which use AI to grade trading cards, but as far as I can tell the only attempt for comics is prompting ChatGPT. The problem is, huge multimodal models aren't that good at grading comics. 

I tried Claude (3.7 Sonnet) and ChatGPT on two older (1960s) 9.4s with good scans of the front and back cover. ChatGPT guessed 6.0 for both (8 grades off!), while Claude guessed 7.5 and 5.5-6.0. The language aspect was excellent, creating very reasonable sounding explanations, but a better word here would be "hallucination". Not to knock their overall vision capabilities, there's just something about the task in combination with their training method and data that makes it one of their weakest areas.

Well, why hasn't someone else tried AI grading comic books? That's generally a pretty important question to ask when trying something that hasn't been done before, especially if you don't *really* know what you're doing.

1. There's no public dataset for images of physical comic books
- It's pretty easy to find images because so many people sell comics on the internet

2. Several images are necessary per comic
- This complicates things quite a bit, especially since the image count will be variable. As a start you can just use the whole front and back covers, maybe even joining them side-by-side to form a single image. 

3. Advancements in ML and GPUs
- But excluding vision-language 

4. Comic covers have a lot of art and color

What I mean by that last point is that if you don't have a reference copy available, there's a level of "intelligence" required to . It's the type of thing effortless/subconscious for humans, but difficult for computers and nearly impossible without deep learning. 

The image on the right is zoomed in at and above the "R". Even though the number of pixels the defect takes up is the same, without the surrounding context it's much more difficult to tell whether the top half has any defects. The lower portion is still somewhat trivial, since it's rare for a shape like that to occur naturally. 

![](detail-context.png)
![](detail.png)

# Making the dataset

To start I used 142k scans of 71k CGC (or CBCS) graded comics. Of course, if you give any model full images it'll just look at the big grade number at the top left, so you need to crop that out and ideally the other sides to remove as many meaningless pixels as possible.

I tried a few approaches using OpenCV, which is a more traditional computer vision library. Finding a big rectangle inside a bigger rectangle befuddled them all in a decent % of cases.

I had some crazy ideas which required very accurate crops of the comic, so I wrote a custom algorithm to generate candidate edges from all 4 directions, 

The bottom of the plastic casing has a lot of color changes before the comic which proved difficult to sort out since it's not very consistent


Most of the grade impact is in the 10% of pixels at the edges of each comic

# EDA

[Show a graph of grade by year (5 at a time) and also label type. Before and after filtering for ML input]

Year is extremely correlated. Cheaper books aren't worth grading unless a high grade is expected (often 9.8 or bust)



# Pre-trained models

All the models I've used so far had already been trained on a ton of natural images. Even though most of what they learned isn't applicable to the task, almost anything is better than starting from scratch. 

The DINO method <https://arxiv.org/abs/2104.14294> is really interesting, and I found models trained using it to perform very well. Just training a simple classifier on top of a pre-trained models 384 output  

The approach of not using any label is quite fascinating. As a crude analogy, compare it to giving a bunch of paintings to someone who doesn't know much about art, and having them sort it into groups/piles. 

If you gave several versions of painting, with random flipping/mirroring, cropping, hue changes, etc. they'll probably do worse, but are forced to learn more robust ways to categorize paintings. Maybe you merge two paintings somehow, and expect them to place it somewhere in-between the two groups. 

If instead of one person, it was closer to a game of telephone where nobody spoke a common language, they'd probably learn to just focus on the important parts

# Benchmark

The simplest metric I'm using is mean absolute error (MAE), basically how far off a guess is on average on the 0.5-10 scale. If I had to guess, 

Guessing the dataset Avg - 

Guessing based on each comic's publishing year - 1.02 MAE

Forums (151 high quality, different from dataset) - 0.80 (individual) 0.74 (group averaged) 

Conrad (~50 full-size) - 0.55

dinov2-base (512x512 8 cols)   Pro71k - 0.51 
dinov2-base (256x512 per img)  Pro71k -  
dinov2-large (384x768 per img) Pro71k -

it's always good to start with an exceedingly simple baseline. Guessing a year's average on a training dataset of just CGC comics results in an MAE of 1.02. 

The next logical step is to see how well people do, and luckily there's an entire forum of people asking for grades, often posting the official results when they come back from CGC. The only potential issue is that the average comic posted there could be different, since people  are biased towards posting their most expensive submissions or anything particularly "tough" to grade.

for 151 guesses over 34 posts the MAE was 0.80, and taking the average guess for each post results in a MAE of 0.74

You can see how well you do on the dataset here <https://www.conradkay.com/grade>

A validation set. This should usually be different in some way than what model trains on, but still representative of you problem. I made it have a more balanced distribution of grades, and with less correlation between year and grade

My MAE on the validation set is around 0.55


# Grad-CAM

![The front and back cover happen to be almost identical for this comic, so it looks a bit like certain columns were duplicated](cam.png)

Generating these visualizations is fairly simple, in both code and computation:

- We force PyTorch to do some calculus measuring much each region impacts the probability it gives for a specific grade
- That can be normalized and reshaped into a grayscale image matching our input image's dimensions
- OpenCV makes it easy to use a colormap (Turbo) so that instead of displaying black to white, it shows blue to yellow to red 

These give a way to basically check the model's work. If it misses anything that seems significant, the grade is probably lower than it predicts. The model paying attention to things which aren't defects is more nuanced. Maybe for a flawless comic, sharp corners might be the most important thing it sees. 

# More Data

So far the model has only been trained on already CGC/CBCS graded comics

Imagine we take several different graders . We can get a bias score, and a consistency score. It doesn't matter much how accurate the model is, as long as it's grades average out to being correct it will have a bias of 0, and we can arbitrarily set a consistency of 1 as equaling the model.

Basically we can create a model to convert one of their grades into the probabilities our model would output on average. So if they gave a comic an 8.5, we might output 40% 8.5, 30% 9.0, 15% 8.0, 0.4% 9.6, etc. 

Essentially, that converts their grading format-- which is usually the same 0.5-10, but often less precise or biased in a specific direction--into CGC's format. It's fairly unlikely. The only thing to really worry about is more specific biases. Training on worse/less applicable data first is a common theme in ML, and should greatly mitigate this. 

(bucketed how many listings belong to sellers with x-y total listings)

An easy to increase data quality is to look at the biggest losses manually and decide whether to remove, relabel, or keep for future training runs. At the very least, this makes it very apparent what the model's limitations and weaknesses are.